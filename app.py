import streamlit as st
from sentence_transformers import SentenceTransformer
import faiss
import pickle
import os
import numpy as np
import time
import openai
from io import BytesIO

# === Streamlit Config ===
st.set_page_config(page_title="BhavBot - Bhavna's Resume Bot", page_icon="ü§ñ")

# === Constants ===
FAISS_INDEX_FILE = "faiss.index"
METADATA_FILE = "metadata.pkl"
MAX_REQUESTS_PER_HOUR = 10
RATE_LIMIT_KEY = "rate_limit"

# === OpenAI API Key ===
openai.api_key = st.secrets["OPENAI_API_KEY"]

# === Load API Key ===
if not openai.api_key:
    st.error("OpenAI API key not found. Please add it in Streamlit Secrets or environment variables.")
    st.stop()


# === General query detection ===
def is_general_query(query):
    general_phrases = ["hi", "hello", "hey", "how are you", "what‚Äôs up", "who are you", "tell me about yourself"]
    return query.strip().lower() in general_phrases

# === Load FAISS index ===
@st.cache_resource(show_spinner=False)
def load_faiss_index():
    print("üîß Loading FAISS index...")
    t0 = time.time()
    index = faiss.read_index(FAISS_INDEX_FILE)
    print(f"‚úÖ FAISS index loaded in {time.time() - t0:.2f} sec")
    return index

# === Load metadata ===
@st.cache_resource(show_spinner=False)
def load_metadata():
    print("üìö Loading metadata...")
    t0 = time.time()
    with open(METADATA_FILE, "rb") as f:
        data = pickle.load(f)
    print(f"‚úÖ Metadata loaded in {time.time() - t0:.2f} sec. Entries: {len(data)}")
    return data

# === Load embedding model ===
@st.cache_resource(show_spinner=False)
def load_model():
    print("üß† Loading embedding model...")
    t0 = time.time()
    try:
        model = SentenceTransformer("paraphrase-MiniLM-L6-v2")
        print(f"‚úÖ Model loaded in {time.time() - t0:.2f} sec")
        return model
    except Exception as e:
        print(f"‚ùå Error loading model: {e}")
        raise RuntimeError("Failed to load SentenceTransformer model.")

# === Similarity Search with Education Fallback ===
def similarity_search(query: str, k: int = 10):
    print("üîç Running similarity search...")
    t0 = time.time()
    query_embedding = embedding_model.encode([query])
    query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)

    scores, indices = faiss_index.search(query_embedding.astype("float32"), k=k)

    retrieved_chunks = []
    retrieved_scores = []

    for score, idx in zip(scores[0], indices[0]):
        if 0 <= idx < len(metadata_store):
            retrieved_chunks.append(metadata_store[idx].get("text", ""))
            retrieved_scores.append(score)

    # ‚úÖ If query is education-related, append all university chunks
    if any(word in query.lower() for word in ["education", "university", "degree", "college"]):
        edu_chunks = [m["text"] for m in metadata_store if "university" in m["text"].lower() or "bachelor" in m["text"].lower()]
        for ec in edu_chunks:
            if ec not in retrieved_chunks:
                retrieved_chunks.append(ec)
                retrieved_scores.append(0.0)  # Dummy score
        print("üéì Education-related query detected ‚Äî merged all university chunks.")

    print(f"‚úÖ Retrieved {len(retrieved_chunks)} chunks in {time.time() - t0:.2f}s")
    
    # Debugging: print retrieved chunks
    print("\n=== üîç FAISS Retrieved Chunks ===")
    for i, (chunk, score) in enumerate(zip(retrieved_chunks, retrieved_scores)):
        print(f"[{i}] Score: {score:.4f}\n{chunk[:200]}...\n")

    return retrieved_chunks, retrieved_scores, indices

# === Rate Limiting ===
def check_rate_limit():
    now = time.time()
    data = st.session_state.get(RATE_LIMIT_KEY, {"count": 0, "start_time": now})
    if now - data["start_time"] > 3600:
        data = {"count": 0, "start_time": now}
    if data["count"] >= MAX_REQUESTS_PER_HOUR:
        return False
    data["count"] += 1
    st.session_state[RATE_LIMIT_KEY] = data
    return True

# === Load all resources ===

col1, col2, col3 = st.columns([1, 2, 1])
with col2:
    st.image("https://raw.githubusercontent.com/UnicornBhavna/Chatbots/main/pic.jpeg", caption="Bhavna Lal", width=180)

# Inject custom CSS for full-page background color

page_bg = """
<style>
    body {
        background-color: #FFE4E1; /* baby light pink */
    }
    [data-testid="stAppViewContainer"] {
        background-color: #FFE4E1 !important;
    }
    [data-testid="stHeader"] {
        background-color: #FFE4E1 !important;
    }
    [data-testid="stSidebar"] {
        background-color: #FFE4E1 !important;
    }
</style>
"""
st.markdown(page_bg, unsafe_allow_html=True)


with st.spinner("‚è≥ Loading FAISS index..."):
    faiss_index = load_faiss_index()

with st.spinner("‚è≥ Loading metadata..."):
    metadata_store = load_metadata()

with st.spinner("‚è≥ Loading embedding model (may take time on first run)..."):
    try:
        embedding_model = load_model()
    except Exception as e:
        st.error("‚ùå Failed to load embedding model. Please try again later.")
        st.stop()

# === UI ===
st.title("üë©‚Äçüíª BhavBot - Bhavna's Resume Chatbot")
st.markdown("Ask about Bhavna's experience, education, skills, or leadership roles. üí° *Tip: I‚Äôm BhavBot, your friendly resume assistant!*")

query = st.text_input("üì® Ask a question about Bhavna's resume:")

if query:

    if not check_rate_limit():
        st.warning(f"‚ö†Ô∏è You‚Äôve hit the limit of {MAX_REQUESTS_PER_HOUR} questions/hour. Please wait and try again later.")

    elif is_name_query(query):
        st.markdown("### ‚úÖ Answer:")
        st.write("Bhavna")


    elif is_general_query(query):
        with st.spinner("üí¨ Generating a friendly response..."):
            try:
                response = openai.ChatCompletion.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": "You are BhavBot, a friendly AI assistant who answers questions about Bhavna's resume and introduces yourself as BhavBot when asked who you are. If asked for your name, reply with 'Bhavna'."},
                        {"role": "user", "content": query}
                    ],
                    max_tokens=150,
                    temperature=0.3
                )
                answer = response.choices[0].message.content.strip()
            except Exception as e:
                answer = f"‚ùå Error: {e}"
        st.markdown("### ‚úÖ Answer:")
        st.write(answer)

    else:
        with st.spinner("üîç Searching relevant resume snippets..."):
            matched_chunks, scores, indices = similarity_search(query)
            context = "\n\n---\n\n".join(matched_chunks)

        with st.spinner("‚úçÔ∏è Generating answer..."):
            try:
                prompt = f"""You are BhavBot, Bhavna's AI resume assistant.

You ONLY answer using the provided resume snippets.  
If asked who you are, you introduce yourself as BhavBot, Bhavna's friendly AI assistant.  
Do not make up information.

--- Resume Snippets ---
{context}

--- Question ---
{query}

If the answer is not found in the resume, reply: "This information is not available in the resume. Please contact Bhavna directly for more details."
"""
                response = openai.ChatCompletion.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": "You are BhavBot, a friendly AI assistant who answers based only on Bhavna's resume."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=500,
                    temperature=0.2
                )
                answer = response.choices[0].message.content.strip()
            except Exception as e:
                answer = f"‚ùå Error generating answer: {e}"
                print(f"Error during OpenAI call: {e}")

        st.markdown("### ‚úÖ Answer:")
        st.write(answer)

#        with st.expander("üìÑ Show Resume Snippets Used"):
#            for chunk, score in zip(matched_chunks, scores):
#                st.markdown(f"**Score**: {score:.4f}")
#                st.code(chunk)



## Download PDF

url = "https://raw.githubusercontent.com/UnicornBhavna/Chatbots/main/Bhavna.pdf"

# Fetch PDF bytes
response = requests.get(url)

if response.status_code == 200:
    pdf_bytes = BytesIO(response.content)

    st.download_button(
        label="üìÑ Download Resume (PDF)",
        data=pdf_bytes,
        file_name="Bhavna_Resume.pdf",
        mime="application/pdf"
    )
else:
    st.error("‚ö†Ô∏è Could not load the PDF from GitHub. Please check the URL.")

st.download_button(
    label="üìÑ Download PDF",
    data=pdf_bytes,
    file_name="Bhavna_Resume.pdf",
    mime="application/pdf"
)

st.markdown(
    """
    <style>
    .stDownloadButton button {
        background-color: #FFC0CB;
        color: black;
        border-radius: 10px;
        font-weight: bold;
    }
    </style>
    """,
    unsafe_allow_html=True
)
